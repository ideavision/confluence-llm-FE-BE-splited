# This env template shows how to configure Payserai for multilingual use
# In this case, it is configured for French and English
# To use it, copy it to .env in the docker_compose directory.
# Feel free to combine it with the other templates to suit your needs

# Rephrase the user query in specified languages using LLM, use comma separated values
MULTILINGUAL_QUERY_EXPANSION=""


# A recent MIT license multilingual model: https://huggingface.co/intfloat/multilingual-e5-small
DOCUMENT_ENCODER_MODEL="intfloat/multilingual-e5-small"

# The model above is trained with the following prefix for queries and passages to improve retrieval
# by letting the model know which of the two type is currently being embedded
ASYM_QUERY_PREFIX="query: "
ASYM_PASSAGE_PREFIX="passage: "

# Depends model by model, the one shown above is tuned with this as True
NORMALIZE_EMBEDDINGS="True"

# Use LLM to determine if chunks are relevant to the query
# May not work well for languages that do not have much training data in the LLM training set
# If using a common language like Spanish, French, Chinese, etc. this can be kept turned on
DISABLE_LLM_CHUNK_FILTER="True"

# The default reranking models are English first
# There are no great quality French/English reranking models currently so turning this off
ENABLE_RERANKING_ASYNC_FLOW="False"
ENABLE_RERANKING_REAL_TIME_FLOW="False"

# Enables fine-grained embeddings for better retrieval
# At the cost of indexing speed (~5x slower), query time is same speed
# Since reranking is turned off and multilingual retrieval is generally harder
# it is advised to turn this one on
ENABLE_MINI_CHUNK="True"

# Using a stronger LLM will help with multilingual tasks
# Since documents may be in multiple languages, and there are additional instructions to respond
# in the user query's language, it is advised to use the best model possible

GEN_AI_MODEL_VERSION="gpt-4o"

CONFLUENCE_CONNECTOR_LABELS_TO_INCLUDE="gpt-hr,gpt-aml,gpt-kyc,gpt-compliance,gpt-dev,gpt-servers"

# ### PostgreSQL
# # PostgreSQL username
# POSTGRES_USER=${POSTGRES_USER:-postgres}
# # PostgreSQL password
# POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-password}
    
# ### Auth Settings
# # Type of authentication to use (e.g., disabled, google_oauth)
# AUTH_TYPE=${AUTH_TYPE:-google_oauth}
# # Time in seconds before a user session expires
# SESSION_EXPIRE_TIME_SECONDS=${SESSION_EXPIRE_TIME_SECONDS:-}
# # Comma-separated list of allowed email domains for user registration
# VALID_EMAIL_DOMAINS=${VALID_EMAIL_DOMAINS:-}
# # Google OAuth client ID for authentication
# GOOGLE_OAUTH_CLIENT_ID=${GOOGLE_OAUTH_CLIENT_ID:-}
# # Google OAuth client secret for authentication
# GOOGLE_OAUTH_CLIENT_SECRET=${GOOGLE_OAUTH_CLIENT_SECRET:-}
# # Whether email verification is required for basic auth
# REQUIRE_EMAIL_VERIFICATION=${REQUIRE_EMAIL_VERIFICATION:-}
# # SMTP server for sending verification emails (default: smtp.gmail.com)
# SMTP_SERVER=${SMTP_SERVER:-}
# # SMTP port for sending verification emails (default: 587)
# SMTP_PORT=${SMTP_PORT:-}
# # SMTP username for sending verification emails
# SMTP_USER=${SMTP_USER:-}
# # SMTP password for sending verification emails
# SMTP_PASS=${SMTP_PASS:-}

# ### Gen AI Settings
# # Provider of the generative AI model (e.g., openai)
# GEN_AI_MODEL_PROVIDER=${GEN_AI_MODEL_PROVIDER:-openai}
# # Version of the generative AI model to use
# GEN_AI_MODEL_VERSION=${GEN_AI_MODEL_VERSION:-gpt-4o}
# # Faster alternative model for secondary flows
# FAST_GEN_AI_MODEL_VERSION=${FAST_GEN_AI_MODEL_VERSION:-gpt-4o}
# # API key for accessing the generative AI service
# GEN_AI_API_KEY=${GEN_AI_API_KEY:-}
# # Endpoint URL for the generative AI API
# GEN_AI_API_ENDPOINT=${GEN_AI_API_ENDPOINT:-}
# # Version of the generative AI API being used
# GEN_AI_API_VERSION=${GEN_AI_API_VERSION:-}
# # LiteLLM custom provider type
# GEN_AI_LLM_PROVIDER_TYPE=${GEN_AI_LLM_PROVIDER_TYPE:-}
# # Timeout for question-answering operations in seconds
# QA_TIMEOUT=${QA_TIMEOUT:-}
# # Number of document tokens fed to the generative model
# NUM_DOCUMENT_TOKENS_FED_TO_GENERATIVE_MODEL=${NUM_DOCUMENT_TOKENS_FED_TO_GENERATIVE_MODEL:-}
# # Disable LLM filter extraction feature
# DISABLE_LLM_FILTER_EXTRACTION=${DISABLE_LLM_FILTER_EXTRACTION:-}
# # Disable LLM chunk filtering feature
# DISABLE_LLM_CHUNK_FILTER=${DISABLE_LLM_CHUNK_FILTER:-}
# # Disable LLM search choice feature
# DISABLE_LLM_CHOOSE_SEARCH=${DISABLE_LLM_CHOOSE_SEARCH:-}
# # Completely disable generative AI features
# DISABLE_GENERATIVE_AI=${DISABLE_GENERATIVE_AI:-}

# ### Query Options
# # Controls the recency bias for search results (decay over years)
# DOC_TIME_DECAY=${DOC_TIME_DECAY:-}
# # Balances between vector and keyword search (0 for keyword, 1 for vector)
# HYBRID_ALPHA=${HYBRID_ALPHA:-}
# # Enable or disable keyword query editing
# EDIT_KEYWORD_QUERY=${EDIT_KEYWORD_QUERY:-}
# # Specify languages for query expansion
# MULTILINGUAL_QUERY_EXPANSION=${MULTILINGUAL_QUERY_EXPANSION:-}
# # Override the default question-answering prompt format
# QA_PROMPT_OVERRIDE=${QA_PROMPT_OVERRIDE:-}

# ### Other services
# # Host for the PostgreSQL database
# POSTGRES_HOST=relational_db
# # Host for the Vespa search engine
# VESPA_HOST=index
# # Web domain used for frontend redirect authentication
# WEB_DOMAIN=${WEB_DOMAIN:-}

# ### NLP model configs
# # Model used for document encoding
# DOCUMENT_ENCODER_MODEL=${DOCUMENT_ENCODER_MODEL:-}
# # Enable or disable normalization of embeddings
# NORMALIZE_EMBEDDINGS=${NORMALIZE_EMBEDDINGS:-}
# # Prefix for asymmetric query retrievals
# ASYM_QUERY_PREFIX=${ASYM_QUERY_PREFIX:-}
# # Enable or disable real-time reranking
# ENABLE_RERANKING_REAL_TIME_FLOW=${ENABLE_RERANKING_REAL_TIME_FLOW:-}
# # Enable or disable asynchronous reranking
# ENABLE_RERANKING_ASYNC_FLOW=${ENABLE_RERANKING_ASYNC_FLOW:-}
# # Host for the model server
# MODEL_SERVER_HOST=${MODEL_SERVER_HOST:-}
# # Port for the model server
# MODEL_SERVER_PORT=${MODEL_SERVER_PORT:-}

# ### Logging and Telemetry
# # Disable anonymous usage telemetry
# DISABLE_TELEMETRY=${DISABLE_TELEMETRY:-}
# # Set logging level (e.g., info, debug)
# LOG_LEVEL=${LOG_LEVEL:-info}
# # Enable logging of all model prompts and outputs
# LOG_ALL_MODEL_INTERACTIONS=${LOG_ALL_MODEL_INTERACTIONS:-}
# # Enable logging of Vespa query performance information
# LOG_VESPA_TIMING_INFORMATION=${LOG_VESPA_TIMING_INFORMATION:-}

# ### NLP model configs
# # Model used for document encoding
# DOCUMENT_ENCODER_MODEL=${DOCUMENT_ENCODER_MODEL:-}
# # Enable or disable normalization of embeddings
# NORMALIZE_EMBEDDINGS=${NORMALIZE_EMBEDDINGS:-}
# # Prefix for asymmetric query retrievals
# ASYM_QUERY_PREFIX=${ASYM_QUERY_PREFIX:-}
# # Prefix for asymmetric passage retrievals
# ASYM_PASSAGE_PREFIX=${ASYM_PASSAGE_PREFIX:-}
# # Host for the model server
# MODEL_SERVER_HOST=${MODEL_SERVER_HOST:-}
# # Port for the model server
# MODEL_SERVER_PORT=${MODEL_SERVER_PORT:-}
# # Host for the indexing model server
# INDEXING_MODEL_SERVER_HOST=${INDEXING_MODEL_SERVER_HOST:-}
# # Minimum number of threads for ML models
# MIN_THREADS_ML_MODELS=${MIN_THREADS_ML_MODELS:-}

# ### Indexing Configs
# # Number of worker processes for indexing
# NUM_INDEXING_WORKERS=${NUM_INDEXING_WORKERS:-}
# # Enable or disable Dask job client
# DASK_JOB_CLIENT_ENABLED=${DASK_JOB_CLIENT_ENABLED:-}
# # Continue indexing on connector failure
# CONTINUE_ON_CONNECTOR_FAILURE=${CONTINUE_ON_CONNECTOR_FAILURE:-}
# # Enable experimental checkpointing feature
# EXPERIMENTAL_CHECKPOINTING_ENABLED=${EXPERIMENTAL_CHECKPOINTING_ENABLED:-}

# ### Logging and Telemetry
# # Disable anonymous usage telemetry
# DISABLE_TELEMETRY=${DISABLE_TELEMETRY:-}
# # Set logging level (e.g., info, debug)
# LOG_LEVEL=${LOG_LEVEL:-info}
# # Enable logging of all model prompts and outputs
# LOG_ALL_MODEL_INTERACTIONS=${LOG_ALL_MODEL_INTERACTIONS:-}
# # Enable logging of Vespa query performance information
# LOG_VESPA_TIMING_INFORMATION=${LOG_VESPA_TIMING_INFORMATION:-}

# ### Model
# # Model used for document encoding
# DOCUMENT_ENCODER_MODEL=${DOCUMENT_ENCODER_MODEL:-}
# # Enable or disable normalization of embeddings
# NORMALIZE_EMBEDDINGS=${NORMALIZE_EMBEDDINGS:-}
# # Minimum number of threads for ML models
# MIN_THREADS_ML_MODELS=${MIN_THREADS_ML_MODELS:-}


